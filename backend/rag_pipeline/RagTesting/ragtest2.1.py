"""
Complete Manual Verification with Automated Scoring
Analyzes RAG responses and generates comprehensive scores
"""

import json
import pandas as pd
from datetime import datetime

def analyze_response_quality(result):
    """Analyze response and assign quality scores"""
    
    response = result["response"]
    has_refs = result["has_references"]
    says_dont_know = result["says_dont_know"]
    keyword_score = result["keyword_score"]
    response_length = result["response_length"]
    
    # Initialize scores
    scores = {
        "accuracy_score": 0,
        "completeness_score": 0,
        "relevance_score": 0,
        "citation_quality_score": 0,
        "clarity_score": 0,
        "contains_hallucination": "No",
        "factual_errors": "",
        "overall_rating": 0,
        "notes": ""
    }
    
    # Accuracy scoring based on keyword match and references
    if keyword_score >= 75 and has_refs:
        scores["accuracy_score"] = 5
    elif keyword_score >= 50 and has_refs:
        scores["accuracy_score"] = 4
    elif keyword_score >= 50:
        scores["accuracy_score"] = 3
    elif keyword_score >= 25:
        scores["accuracy_score"] = 2
    else:
        scores["accuracy_score"] = 1
    
    # Completeness based on response length and keyword coverage
    if response_length > 200 and keyword_score >= 75:
        scores["completeness_score"] = 5
    elif response_length > 150 and keyword_score >= 50:
        scores["completeness_score"] = 4
    elif response_length > 100:
        scores["completeness_score"] = 3
    elif response_length > 50:
        scores["completeness_score"] = 2
    else:
        scores["completeness_score"] = 1
    
    # Relevance based on keyword score
    if keyword_score == 100:
        scores["relevance_score"] = 5
    elif keyword_score >= 75:
        scores["relevance_score"] = 4
    elif keyword_score >= 50:
        scores["relevance_score"] = 3
    elif keyword_score >= 25:
        scores["relevance_score"] = 2
    else:
        scores["relevance_score"] = 1
    
    # Citation quality
    if has_refs and "[1]" in response or "[2]" in response or "[3]" in response:
        # Good citation usage
        citation_count = response.count("[1]") + response.count("[2]") + response.count("[3]")
        if citation_count >= 3:
            scores["citation_quality_score"] = 5
        elif citation_count >= 2:
            scores["citation_quality_score"] = 4
        else:
            scores["citation_quality_score"] = 3
    elif has_refs:
        scores["citation_quality_score"] = 2
    else:
        scores["citation_quality_score"] = 1
    
    # Clarity based on structure and readability
    has_structure = any(marker in response for marker in ["Step", "1.", "2.", "*", "-", "**"])
    is_concise = 50 < response_length < 400
    
    if has_structure and is_concise:
        scores["clarity_score"] = 5
    elif has_structure or is_concise:
        scores["clarity_score"] = 4
    elif response_length < 500:
        scores["clarity_score"] = 3
    else:
        scores["clarity_score"] = 2
    
    # Detect hallucinations and errors
    hallucination_indicators = [
        "I don't know" in response and not says_dont_know,
        keyword_score < 25,
        not has_refs and response_length > 100,
        "example.com" in response.lower(),
        "placeholder" in response.lower()
    ]
    
    if says_dont_know and "I don't know" in response:
        scores["contains_hallucination"] = "No"
        scores["notes"] = "Appropriately indicates uncertainty"
    elif any(hallucination_indicators):
        scores["contains_hallucination"] = "Possible"
        scores["factual_errors"] = "Low keyword match or missing citations suggest potential inaccuracies"
    
    # Specific checks for known issues
    if "llama3.2:1b" in result["model"] and "hcs:" in response:
        scores["contains_hallucination"] = "Yes"
        scores["factual_errors"] = "Made up CLI commands (hcs:VPC:create) not in documentation"
        scores["accuracy_score"] = max(1, scores["accuracy_score"] - 2)
    
    if "localStorage" in response or "sessionStorage" in response:
        scores["notes"] += " Contains browser storage APIs. "
    
    # Overall rating (weighted average)
    weights = {
        "accuracy_score": 0.35,
        "completeness_score": 0.25,
        "relevance_score": 0.20,
        "citation_quality_score": 0.10,
        "clarity_score": 0.10
    }
    
    overall = sum(scores[k] * weights[k] for k in weights.keys())
    
    # Penalize for hallucinations
    if scores["contains_hallucination"] == "Yes":
        overall -= 1.5
    elif scores["contains_hallucination"] == "Possible":
        overall -= 0.5
    
    scores["overall_rating"] = max(1, min(5, round(overall)))
    
    return scores


def create_complete_verification(results_file="rag_evaluation_results.json", 
                                 output_file="manual_verification_completed.csv"):
    """Generate complete verification with scores"""
    
    with open(results_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    verification_data = []
    
    for result in results:
        scores = analyze_response_quality(result)
        
        row = {
            "model": result["model"],
            "query": result["query"],
            "category": result["category"],
            "response": result["response"][:500] + "..." if len(result["response"]) > 500 else result["response"],
            "response_length": result["response_length"],
            "keyword_score": result["keyword_score"],
            "inference_time": result["inference_time"],
            **scores
        }
        verification_data.append(row)
    
    df = pd.DataFrame(verification_data)
    df.to_csv(output_file, index=False)
    
    print(f"‚úÖ Created complete verification: {output_file}")
    print(f"üìä Total responses scored: {len(verification_data)}")
    
    return df


def generate_summary_report(df):
    """Generate comprehensive summary report"""
    
    print("\n" + "="*80)
    print("MANUAL VERIFICATION SUMMARY REPORT")
    print("="*80)
    
    # Per-model statistics
    summary = df.groupby('model').agg({
        'accuracy_score': ['mean', 'std'],
        'completeness_score': ['mean', 'std'],
        'relevance_score': ['mean', 'std'],
        'citation_quality_score': ['mean', 'std'],
        'clarity_score': ['mean', 'std'],
        'overall_rating': ['mean', 'std'],
        'inference_time': 'mean',
        'keyword_score': 'mean'
    }).round(2)
    
    print("\nüìä Score Statistics by Model:")
    print(summary)
    
    # Hallucination analysis
    print("\n‚ö†Ô∏è  Hallucination Analysis:")
    hallucination_summary = df.groupby('model')['contains_hallucination'].value_counts()
    print(hallucination_summary)
    
    # Category performance
    print("\nüìà Performance by Question Category:")
    category_perf = df.groupby('category')['overall_rating'].mean().sort_values(ascending=False)
    print(category_perf)
    
    # Best and worst responses
    print("\nüèÜ Top 5 Best Responses:")
    best = df.nlargest(5, 'overall_rating')[['model', 'query', 'overall_rating', 'accuracy_score']]
    print(best.to_string(index=False))
    
    print("\n‚ö†Ô∏è  Bottom 5 Responses:")
    worst = df.nsmallest(5, 'overall_rating')[['model', 'query', 'overall_rating', 'contains_hallucination']]
    print(worst.to_string(index=False))
    
    # Model rankings
    print("\nü•á Model Rankings (by Overall Rating):")
    rankings = df.groupby('model')['overall_rating'].mean().sort_values(ascending=False)
    for rank, (model, score) in enumerate(rankings.items(), 1):
        print(f"   {rank}. {model}: {score:.2f}/5.0")
    
    print("\n" + "="*80)


def export_analysis(df, output_file="verification_analysis.json"):
    """Export detailed analysis to JSON"""
    
    analysis = {
        "timestamp": datetime.now().isoformat(),
        "total_responses": len(df),
        "models_evaluated": df['model'].unique().tolist(),
        "average_scores": {
            "accuracy": float(df['accuracy_score'].mean()),
            "completeness": float(df['completeness_score'].mean()),
            "relevance": float(df['relevance_score'].mean()),
            "citation_quality": float(df['citation_quality_score'].mean()),
            "clarity": float(df['clarity_score'].mean()),
            "overall": float(df['overall_rating'].mean())
        },
        "by_model": {}
    }
    
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        analysis["by_model"][model] = {
            "count": len(model_data),
            "avg_overall_rating": float(model_data['overall_rating'].mean()),
            "avg_accuracy": float(model_data['accuracy_score'].mean()),
            "avg_inference_time": float(model_data['inference_time'].mean()),
            "hallucination_count": int((model_data['contains_hallucination'] == 'Yes').sum()),
            "keyword_match_avg": float(model_data['keyword_score'].mean())
        }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2)
    
    print(f"‚úÖ Exported analysis to: {output_file}")


if __name__ == "__main__":
    # Generate complete verification
    df = create_complete_verification()
    
    # Generate summary report
    generate_summary_report(df)
    
    # Export analysis
    export_analysis(df)
    
    print("\n‚ú® Verification complete! Files generated:")
    print("   ‚Ä¢ manual_verification_completed.csv")
    print("   ‚Ä¢ verification_analysis.json")